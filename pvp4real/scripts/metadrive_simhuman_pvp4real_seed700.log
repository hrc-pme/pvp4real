/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_stream, resource_exists
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
[38;20m[INFO] Environment: HumanInTheLoopEnv[0m
[38;20m[INFO] MetaDrive version: 0.4.3[0m
[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()][0m
[38;20m[INFO] Render Mode: none[0m
[38;20m[INFO] Horizon (Max steps per agent): 1500[0m
Using cpu device
Wrapping the env with a `Monitor` wrapper
[38;20m[INFO] Assets version: 0.4.3[0m
[38;20m[INFO] Known Pipes: glxGraphicsPipe[0m
[38;20m[INFO] Start Scenario Index: 100, Num Scenarios : 50[0m
INFO:/workspace/pvp4real/pvp/sb3/common/monitor.py:In Monitor, we auto detect the record keys: ('overtake_vehicle_num', 'velocity', 'steering', 'acceleration', 'step_energy', 'episode_energy', 'policy', 'navigation_command', 'navigation_forward', 'navigation_left', 'navigation_right', 'action', 'raw_action', 'crash_vehicle', 'crash_object', 'crash_building', 'crash_human', 'crash_sidewalk', 'out_of_road', 'arrive_dest', 'max_step', 'env_seed', 'crash', 'step_reward', 'route_completion', 'cost', 'total_cost', 'episode_reward', 'episode_length', 'takeover_start', 'takeover', 'takeover_cost', 'total_takeover_cost', 'native_cost', 'episode_native_cost', 'total_takeover_count', 'ep_overtake_vehicle_num', 'ep_velocity', 'ep_steering', 'ep_acceleration', 'ep_step_energy', 'ep_episode_energy', 'ep_policy', 'ep_navigation_command', 'ep_navigation_forward', 'ep_navigation_left', 'ep_navigation_right', 'ep_action', 'ep_raw_action', 'ep_crash_vehicle', 'ep_crash_object', 'ep_crash_building', 'ep_crash_human', 'ep_crash_sidewalk', 'ep_out_of_road', 'ep_arrive_dest', 'ep_max_step', 'ep_env_seed', 'ep_crash', 'ep_step_reward', 'ep_route_completion', 'ep_cost', 'ep_total_cost', 'ep_episode_reward', 'ep_episode_length', 'ep_takeover_start', 'ep_takeover', 'ep_takeover_cost', 'ep_total_takeover_cost', 'ep_native_cost', 'ep_episode_native_cost', 'ep_total_takeover_count')
Wrapping the env in a DummyVecEnv.
Loading checkpoint from /workspace/pvp4real/pvp/experiments/metadrive/egpo/metadrive_pvp_20m_steps!
/workspace/pvp4real/pvp/sb3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.
  warnings.warn(
/workspace/pvp4real/pvp/sb3/common/save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.
  warnings.warn(
/workspace/pvp4real/pvp/sb3/common/save_util.py:440: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  th_object = th.load(file_content, map_location=device)
Model is loaded from /workspace/pvp4real/pvp/experiments/metadrive/egpo/metadrive_pvp_20m_steps!
Trial name is set to:  metadrive_simhuman_pvp4real_freelevel0.95_2026-02-01_14-22-06_173a89c3
[WARNING] Please note that you are not using wandb right now!!!
We start logging training data into /home/zhenghao/pvp/runs/metadrive_simhuman_pvp4real_freelevel0.95/metadrive_simhuman_pvp4real_freelevel0.95_2026-02-01_14-22-06_173a89c3
[38;20m[INFO] Environment: FakeHumanEnv[0m
[38;20m[INFO] MetaDrive version: 0.4.3[0m
[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()][0m
[38;20m[INFO] Render Mode: none[0m
[38;20m[INFO] Horizon (Max steps per agent): 1500[0m
[38;20m[INFO] Assets version: 0.4.3[0m
[38;20m[INFO] Known Pipes: glxGraphicsPipe[0m
[38;20m[INFO] Start Scenario Index: 100, Num Scenarios : 50[0m
INFO:/workspace/pvp4real/pvp/sb3/common/monitor.py:In Monitor, we auto detect the record keys: ('overtake_vehicle_num', 'velocity', 'steering', 'acceleration', 'step_energy', 'episode_energy', 'policy', 'navigation_command', 'navigation_forward', 'navigation_left', 'navigation_right', 'action', 'raw_action', 'crash_vehicle', 'crash_object', 'crash_building', 'crash_human', 'crash_sidewalk', 'out_of_road', 'arrive_dest', 'max_step', 'env_seed', 'crash', 'step_reward', 'route_completion', 'cost', 'total_cost', 'episode_reward', 'episode_length', 'takeover_start', 'takeover', 'takeover_cost', 'total_takeover_cost', 'native_cost', 'episode_native_cost', 'total_takeover_count', 'takeover_log_prob', 'ep_overtake_vehicle_num', 'ep_velocity', 'ep_steering', 'ep_acceleration', 'ep_step_energy', 'ep_episode_energy', 'ep_policy', 'ep_navigation_command', 'ep_navigation_forward', 'ep_navigation_left', 'ep_navigation_right', 'ep_action', 'ep_raw_action', 'ep_crash_vehicle', 'ep_crash_object', 'ep_crash_building', 'ep_crash_human', 'ep_crash_sidewalk', 'ep_out_of_road', 'ep_arrive_dest', 'ep_max_step', 'ep_env_seed', 'ep_crash', 'ep_step_reward', 'ep_route_completion', 'ep_cost', 'ep_total_cost', 'ep_episode_reward', 'ep_episode_length', 'ep_takeover_start', 'ep_takeover', 'ep_takeover_cost', 'ep_total_takeover_cost', 'ep_native_cost', 'ep_episode_native_cost', 'ep_total_takeover_count', 'ep_takeover_log_prob')
/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_stream, resource_exists
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
[38;20m[INFO] Environment: HumanInTheLoopEnv[0m
[38;20m[INFO] MetaDrive version: 0.4.3[0m
[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()][0m
[38;20m[INFO] Render Mode: none[0m
[38;20m[INFO] Horizon (Max steps per agent): 1500[0m
Using cpu device
Wrapping the env with a `Monitor` wrapper
[38;20m[INFO] Assets version: 0.4.3[0m
[38;20m[INFO] Known Pipes: glxGraphicsPipe[0m
[38;20m[INFO] Start Scenario Index: 100, Num Scenarios : 50[0m
INFO:/workspace/pvp4real/pvp/sb3/common/monitor.py:In Monitor, we auto detect the record keys: ('overtake_vehicle_num', 'velocity', 'steering', 'acceleration', 'step_energy', 'episode_energy', 'policy', 'navigation_command', 'navigation_forward', 'navigation_left', 'navigation_right', 'action', 'raw_action', 'crash_vehicle', 'crash_object', 'crash_building', 'crash_human', 'crash_sidewalk', 'out_of_road', 'arrive_dest', 'max_step', 'env_seed', 'crash', 'step_reward', 'route_completion', 'cost', 'total_cost', 'episode_reward', 'episode_length', 'takeover_start', 'takeover', 'takeover_cost', 'total_takeover_cost', 'native_cost', 'episode_native_cost', 'total_takeover_count', 'ep_overtake_vehicle_num', 'ep_velocity', 'ep_steering', 'ep_acceleration', 'ep_step_energy', 'ep_episode_energy', 'ep_policy', 'ep_navigation_command', 'ep_navigation_forward', 'ep_navigation_left', 'ep_navigation_right', 'ep_action', 'ep_raw_action', 'ep_crash_vehicle', 'ep_crash_object', 'ep_crash_building', 'ep_crash_human', 'ep_crash_sidewalk', 'ep_out_of_road', 'ep_arrive_dest', 'ep_max_step', 'ep_env_seed', 'ep_crash', 'ep_step_reward', 'ep_route_completion', 'ep_cost', 'ep_total_cost', 'ep_episode_reward', 'ep_episode_length', 'ep_takeover_start', 'ep_takeover', 'ep_takeover_cost', 'ep_total_takeover_cost', 'ep_native_cost', 'ep_episode_native_cost', 'ep_total_takeover_count')
Wrapping the env in a DummyVecEnv.
Loading checkpoint from /workspace/pvp4real/pvp/experiments/metadrive/egpo/metadrive_pvp_20m_steps!
/workspace/pvp4real/pvp/sb3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.
  warnings.warn(
/workspace/pvp4real/pvp/sb3/common/save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.
  warnings.warn(
/workspace/pvp4real/pvp/sb3/common/save_util.py:440: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  th_object = th.load(file_content, map_location=device)
Model is loaded from /workspace/pvp4real/pvp/experiments/metadrive/egpo/metadrive_pvp_20m_steps!
[38;20m[INFO] Environment: HumanInTheLoopEnv[0m
[38;20m[INFO] MetaDrive version: 0.4.3[0m
[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()][0m
[38;20m[INFO] Render Mode: none[0m
[38;20m[INFO] Horizon (Max steps per agent): 1500[0m
[38;20m[INFO] Assets version: 0.4.3[0m
[38;20m[INFO] Known Pipes: glxGraphicsPipe[0m
[38;20m[INFO] Start Scenario Index: 1000, Num Scenarios : 50[0m
INFO:/workspace/pvp4real/pvp/sb3/common/monitor.py:In Monitor, we auto detect the record keys: ('overtake_vehicle_num', 'velocity', 'steering', 'acceleration', 'step_energy', 'episode_energy', 'policy', 'navigation_command', 'navigation_forward', 'navigation_left', 'navigation_right', 'action', 'raw_action', 'crash_vehicle', 'crash_object', 'crash_building', 'crash_human', 'crash_sidewalk', 'out_of_road', 'arrive_dest', 'max_step', 'env_seed', 'crash', 'step_reward', 'route_completion', 'cost', 'total_cost', 'episode_reward', 'episode_length', 'takeover_start', 'takeover', 'takeover_cost', 'total_takeover_cost', 'native_cost', 'episode_native_cost', 'total_takeover_count', 'ep_overtake_vehicle_num', 'ep_velocity', 'ep_steering', 'ep_acceleration', 'ep_step_energy', 'ep_episode_energy', 'ep_policy', 'ep_navigation_command', 'ep_navigation_forward', 'ep_navigation_left', 'ep_navigation_right', 'ep_action', 'ep_raw_action', 'ep_crash_vehicle', 'ep_crash_object', 'ep_crash_building', 'ep_crash_human', 'ep_crash_sidewalk', 'ep_out_of_road', 'ep_arrive_dest', 'ep_max_step', 'ep_env_seed', 'ep_crash', 'ep_step_reward', 'ep_route_completion', 'ep_cost', 'ep_total_cost', 'ep_episode_reward', 'ep_episode_length', 'ep_takeover_start', 'ep_takeover', 'ep_takeover_cost', 'ep_total_takeover_cost', 'ep_native_cost', 'ep_episode_native_cost', 'ep_total_takeover_count')
Using cpu device
Wrapping the env in a DummyVecEnv.
Logging to /home/zhenghao/pvp/runs/metadrive_simhuman_pvp4real_freelevel0.95/metadrive_simhuman_pvp4real_freelevel0.95_2026-02-01_14-22-06_173a89c3/metadrive_simhuman_pvp4real_freelevel0.95_1
/workspace/pvp4real/pvp/sb3/common/callbacks.py:333: UserWarning: Training and eval env are not of the same type<pvp.sb3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7c3d84840280> != <pvp.sb3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7c3d84840100>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
